{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML0120EN-3.4-Review-LSTM-CharacterModelling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_mfGeY9FbNuo",
        "A4GStvvHbNup",
        "QBPV3tLubNvq"
      ],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PACW5MnobNuJ",
        "colab_type": "text"
      },
      "source": [
        "<img src = \"https://i.imgur.com/UjutVJd.jpg\" align = \"center\">\n",
        "\n",
        "\n",
        "# Text generation using RNN/LSTM (Character-level)\n",
        "In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdq3corNb22b",
        "colab_type": "text"
      },
      "source": [
        "# Table of contents\n",
        "\n",
        "<div>\n",
        "- <a href=\"#intro\">Introduction</a><br />\n",
        "- <a href=\"#arch\">Architectures</a><br />\n",
        "- <a href=\"#lstm\">Long Short-Term Memory Model (LSTM)</a><br />\n",
        "- <a href=\"#build\">Building a LSTM with TensorFlow</a>\n",
        "</div>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjk6jKSMbNuK",
        "colab_type": "text"
      },
      "source": [
        "This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n",
        "The RNN can then be used to generate text character by character that will look like the original training data. \n",
        "\n",
        "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x07SIHMbNuL",
        "colab_type": "text"
      },
      "source": [
        "First, import the requiered libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPFEVJ75bNuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import codecs\n",
        "import os\n",
        "import collections\n",
        "from six.moves import cPickle\n",
        "import numpy as np\n",
        "#from tensorflow.python.ops import rnn_cell\n",
        "#from tensorflow.python.ops import seq2seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN28hSr6bNuO",
        "colab_type": "text"
      },
      "source": [
        "### Data loader\n",
        "The following cell is a class that help to read data from input file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY_Z7U3fbNuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextLoader():\n",
        "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "        self.encoding = encoding\n",
        "\n",
        "        input_file = os.path.join(data_dir, \"input.txt\")\n",
        "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
        "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
        "\n",
        "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
        "            print(\"reading text file\")\n",
        "            self.preprocess(input_file, vocab_file, tensor_file)\n",
        "        else:\n",
        "            print(\"loading preprocessed files\")\n",
        "            self.load_preprocessed(vocab_file, tensor_file)\n",
        "        self.create_batches()\n",
        "        self.reset_batch_pointer()\n",
        "\n",
        "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
        "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
        "            data = f.read()\n",
        "        counter = collections.Counter(data)\n",
        "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
        "        self.chars, _ = zip(*count_pairs)\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
        "        with open(vocab_file, 'wb') as f:\n",
        "            cPickle.dump(self.chars, f)\n",
        "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
        "        np.save(tensor_file, self.tensor)\n",
        "\n",
        "    def load_preprocessed(self, vocab_file, tensor_file):\n",
        "        with open(vocab_file, 'rb') as f:\n",
        "            self.chars = cPickle.load(f)\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
        "        self.tensor = np.load(tensor_file)\n",
        "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
        "\n",
        "    def create_batches(self):\n",
        "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
        "\n",
        "        # When the data (tensor) is too small, let's give them a better error message\n",
        "        if self.num_batches==0:\n",
        "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
        "\n",
        "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
        "        xdata = self.tensor\n",
        "        ydata = np.copy(self.tensor)\n",
        "        ydata[:-1] = xdata[1:]\n",
        "        ydata[-1] = xdata[0]\n",
        "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
        "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
        "        self.pointer += 1\n",
        "        return x, y\n",
        "\n",
        "    def reset_batch_pointer(self):\n",
        "        self.pointer = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btfca01hbNuR",
        "colab_type": "text"
      },
      "source": [
        "### Parameters\n",
        "#### Batch, number_of_batch, batch_size and seq_length\n",
        "what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  \n",
        "\n",
        "Lets assume the input is this sentence: '__here is an example__'. Then:\n",
        "- txt_length = 18  \n",
        "- seq_length = 3  \n",
        "- batch_size = 2  \n",
        "- number_of_batchs = 18/3*2 = 3\n",
        "- batch = array (['h','e','r'],['e',' ','i'])\n",
        "- sample Seq = 'her'  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwJUzlwTbNuS",
        "colab_type": "text"
      },
      "source": [
        "Ok, now, lets look at a real dataset, with real parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBg_fWRbNuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 50 # RNN sequence length\n",
        "batch_size = 60  # minibatch size, i.e. size of data in each epoch\n",
        "num_epochs = 125 # you should change it to 50 if you want to see a relatively good results\n",
        "learning_rate = 0.002\n",
        "decay_rate = 0.97\n",
        "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
        "num_layers = 2 #number of layers in the RNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7qy5i-YbNuV",
        "colab_type": "text"
      },
      "source": [
        "We download the input file, and print a part of it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNiPhNiybNuW",
        "colab_type": "code",
        "outputId": "39fa2e89-53e7-41a5-c2a6-ccbe637066c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
        "with open('input.txt', 'r') as f:\n",
        "    read_data = f.read()\n",
        "    print read_data[0:100]\n",
        "f.closed"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-09 02:44:01 URL:https://public.boxcloud.com/d/1/b1!pH4oZukfQRqec1J4FKXkpcWIy-XMH12q_W958Nf7lizg0KLr81jYZe3RrhUkx8ZWM0b7wNEEX3oBub5qv0FPq2ewGsnQLDnzabkWJXnCpMN1xNusVmsKBoy4Qhej6GF0usx3KsUryo5r6HJrnapAcMZD9GkIz4E54KwbgV4mgPIyztjqqBS52S8d99-ivk___Oz0RHyhB6Y36Vc-pvqKCdhm4ah5RXPHBfCdf6GRvUHkpWYJMCZtVew5DETWNuKFXxk60a01TyNIO2bBNu3593hPqA90RKGEjF9_BVyn0PNzj5SfLlrtlL20nMlLbQnVkNowUYUDLMz9h9OfO6UWzQVeU_SSxQ3BoWtVq1NWbNJ6HWdlvykIojrXTB87hyYfTSOHMYG-wAtxOfuA6lJGiD_k2Gl62NIqAPgTgK05NB8QV1qtRmUJaUAThNx7PogK7-c00MpO7MZe1p5_CUJEnRCob0-L2cpiE3ByDy5Tn65d9n19ECuQWL6d33uIO6KWZitFmKkAgxky-MP9TpnZ7AmyCTfeVgmgPCGg7IqaHg8E3Z82_2NIkF4yhd_FE_VWHsMWSDx1bdSBg9Lja0UFI3My2YM0dOI32PTJgNP2cTzrBIwr6zl3AdWfmVOuiYKWQEaujBqfQj9UiDb1Jz7rrynYz1rzdDa8lxMBbLp2K17hR4xD7cPTw-uDLsGOhzySNaQz0udYVd1jDdqU47gms2Xu0-yqZEJi2flVaw-YZamjofap3eV3Gc5xHsptH1l9TfUcNNdUrcqpGLSY2zTHf0VAxTgSbthLNBPR2vPr93128E2kyA-XGGSehVVdzpQANxYOw3mYdGhGsnp-gPEiOCZntvI9sy79lZQFDC4G0srdiYl0LebYISW-BtibvQhpYsALlfRK5y2PyhJP42_-5TuQYmYxqQYCVZyU7jiufJImq77P_H5aHkN-lbPhYxK8eGMJ0_fAlH6i56L3SY3W_1FDe3ttXb_MFwQUfMbE0p7newUZN939-xruSI2TkN-FpSM6yfl_oyRdYlYxOyhV-tnKn99KVJri675t7jSn5WY1LplpFMwFq0IgLGsHfivtI5GC9yXXb1WNfGKaFFnp9oQDtsvnLGCIBv5yqsoRUcwJBEu5z8IBZZX4o5kuEg00vuh1silaVruVfEv3XqloQNKCdAyhm2pomR-G5RK93C7FeVCnMr5MDHJq0RshcUZESkW5UqAxst-riojacVv79Hd9MXQCi5Y4TZo2aXus2vCSrhoEoqdmcs8JYxKSKb42xR7xN7wQwYGIPeKWFOqnXq-BOo6SJlQPBIGiPmfubjeY7tU8err47QBDU1kN2i-YMEQQMJu5NbMOj6b87VdrjAKjUkSRzsjZnLzKfHCzS2E4mORmcos./download [1115393/1115393] -> \"input.txt\" [1]\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "text": [
            "2019-08-09 02:58:45 URL:https://public.boxcloud.com/d/1/b1!X8OnV-jC7hvBi7kpiBw5omZlGyrCv2ICSoT8HiabK3OQwLBdbMw1FDqvOqGpGK2zCuJz6SasgGExehmKjz-0ShVsPpCYbTXZiOtGXM-J5kUp0hUr3fVBwECOAe4Cy1UwEp4v9trA7EpQeGaI23TpqAhpTEGP7LZ27eAFb6_JndsCXabuT8t22OdfHwREKqOvP_kTb1orWuQhLqalB6-jbo-0IcRsLSrr5_pXJ9Kg9nUQgcFkXEe4zrYsdlhgsWUtlc878hTA5FlImpg9gdYSB2O_83bqt_yG9MDX9o33eehARPbMi7vtyz6mLWeDhMLJXqz9_ZNtfyD6qrrX0H1cixkilzBqOdpm-WORwfqgDN_c-bFSX62uMQKu1CngElhkXHt-TNc9MClw6lZvI0FwS0R2kUbQd3EGZkj0AhtpPY4CON9CLYoEXd_KIUItu13K2zg2k5D-zsD6GOFGOfBpmSwiXd-78RnofY6KhYu8HTCl3v_Op4DuZ2ComXY8150sZLFBnmBUewsBBsv2Gq_3JVuvREckvqPtCr7LX7TBXc3nL3dEnLiticCWZ51ghd5y0Lf3HuJ-aZnm-mihWx9n3CKh5rg1mG_cOzb2ndGtpzyJPY-wjkcKMkMC1aHg6rWkY-bEY0oIqNen7iHqOI6fzwpD7qhA03iZvLD7zI8viAs_NVTJUsVLdKlxSgjgnCs5RFwC7M3rbkMV-sif8WUVrPaNblKgNN6eZAAmpd5J5XGxBHLqcLWD-s3cypJFRc63PFKjaWl2Va-pfsGJMNCyGGVDPAx9YemwOfzXMt4BWzbGbOwYTOHnenX4laT9GJVPfNrFSVA5jeg-I9L0zJBkjdvLbHNhiksCDDU965NZ-eCTZJqYDaiGtSa31Mn8P6s23UNorKcGqvw4-hQcY_1CHnSlSyIJXambc9YxV7XgmaRkXAnoBn5HR_ZhS2gcE_yArKhM-gIyuKq0AeHuYLiUZSdDPtWAEU5Cb_wrGETD5GOhyMGW6C0c5zdq9j7A_x6piU4Vro3BB8ta8xqKn_jn6P3AG9gaKzOCDzh9BHix4rfu8PfxYjbPKkdxTd5SjSSv7XKVYMqlvWxXhM19vmTkmkGCBDedvwppm66Xoe49wtjkzYPr7FEyNlmrf8qVEz7wFPQckOG-ptwSuRIapzRNGQ90YHMIEb_WDRv1jb8sIpbQE-4kAYOC5P3l3EZHLwtTa7h3-3vL1no1c7l4PGeDhLxvZ14qS0XTBaJKT7fsWsTR6dNAyzaCVCqZHY0-7oqrT5UsnTCsxhw5WQ8IJHgKLWr6BGBYYpo33XyiJAqVw_gYSTzCHZ2Xyth7lsu5yq-lQmPEdUVNeBpBOfnTx6huDT6k4IQHSDtrGobhADtvV4pcoppDK1U./download [1115393/1115393] -> \"input.txt\" [1]\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOO2MUdEbNuY",
        "colab_type": "text"
      },
      "source": [
        "Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBsrCGq_bNuZ",
        "colab_type": "code",
        "outputId": "116a861d-ad91-44b8-87cb-7662d4d2eb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "data_loader = TextLoader('', batch_size, seq_length)\n",
        "vocab_size = data_loader.vocab_size\n",
        "print \"vocabulary size:\" ,data_loader.vocab_size\n",
        "print \"Characters:\" ,data_loader.chars\n",
        "print \"vocab number of 'F':\",data_loader.vocab['F']\n",
        "print \"Character sequences (first batch):\", data_loader.x_batches[0]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading text file\n",
            "vocabulary size: 65\n",
            "Characters: (u' ', u'e', u't', u'o', u'a', u'h', u's', u'r', u'n', u'i', u'\\n', u'l', u'd', u'u', u'm', u'y', u',', u'w', u'f', u'c', u'g', u'I', u'b', u'p', u':', u'.', u'A', u'v', u'k', u'T', u\"'\", u'E', u'O', u'N', u'R', u'S', u'L', u'C', u';', u'W', u'U', u'H', u'M', u'B', u'?', u'G', u'!', u'D', u'-', u'F', u'Y', u'P', u'K', u'V', u'j', u'q', u'x', u'z', u'J', u'Q', u'Z', u'X', u'3', u'&', u'$')\n",
            "vocab number of 'F': 49\n",
            "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
            " [19  4 14 ... 14  9 20]\n",
            " [ 8 20 10 ...  8 10 18]\n",
            " ...\n",
            " [21  2  0 ...  0 21  0]\n",
            " [ 9  7  7 ...  0  2  3]\n",
            " [ 3  7  0 ...  5  9 23]]\n",
            "loading preprocessed files\n",
            "vocabulary size: 65\n",
            "Characters: (u' ', u'e', u't', u'o', u'a', u'h', u's', u'r', u'n', u'i', u'\\n', u'l', u'd', u'u', u'm', u'y', u',', u'w', u'f', u'c', u'g', u'I', u'b', u'p', u':', u'.', u'A', u'v', u'k', u'T', u\"'\", u'E', u'O', u'N', u'R', u'S', u'L', u'C', u';', u'W', u'U', u'H', u'M', u'B', u'?', u'G', u'!', u'D', u'-', u'F', u'Y', u'P', u'K', u'V', u'j', u'q', u'x', u'z', u'J', u'Q', u'Z', u'X', u'3', u'&', u'$')\n",
            "vocab number of 'F': 49\n",
            "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
            " [19  4 14 ... 14  9 20]\n",
            " [ 8 20 10 ...  8 10 18]\n",
            " ...\n",
            " [21  2  0 ...  0 21  0]\n",
            " [ 9  7  7 ...  0  2  3]\n",
            " [ 3  7  0 ...  5  9 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZngrIQFbNub",
        "colab_type": "text"
      },
      "source": [
        "### Input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqvVzqvDbNuc",
        "colab_type": "code",
        "outputId": "8fe5c515-2d77-4e63-8c79-223299894f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "x,y = data_loader.next_batch()\n",
        "x"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[49,  9,  7, ...,  1,  4,  7],\n",
              "       [19,  4, 14, ..., 14,  9, 20],\n",
              "       [ 8, 20, 10, ...,  8, 10, 18],\n",
              "       ...,\n",
              "       [21,  2,  0, ...,  0, 21,  0],\n",
              "       [ 9,  7,  7, ...,  0,  2,  3],\n",
              "       [ 3,  7,  0, ...,  5,  9, 23]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[49,  9,  7, ...,  1,  4,  7],\n",
              "       [19,  4, 14, ..., 14,  9, 20],\n",
              "       [ 8, 20, 10, ...,  8, 10, 18],\n",
              "       ...,\n",
              "       [21,  2,  0, ...,  0, 21,  0],\n",
              "       [ 9,  7,  7, ...,  0,  2,  3],\n",
              "       [ 3,  7,  0, ...,  5,  9, 23]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYSD2grBbNue",
        "colab_type": "code",
        "outputId": "1123fc27-255e-4a00-f83a-628e3280c61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x.shape  #batch_size =60, seq_length=50"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbTPCU5vbNui",
        "colab_type": "text"
      },
      "source": [
        "Here, __y__ is the next character for each character in __x__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhmI98PkbNuj",
        "colab_type": "code",
        "outputId": "75b9fcfd-57f5-4618-c97e-e0c3b3f86896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
              "       [ 4, 14, 22, ...,  9, 20,  5],\n",
              "       [20, 10, 29, ..., 10, 18,  4],\n",
              "       ...,\n",
              "       [ 2,  0,  6, ..., 21,  0,  6],\n",
              "       [ 7,  7,  4, ...,  2,  3,  0],\n",
              "       [ 7,  0, 33, ...,  9, 23,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
              "       [ 4, 14, 22, ...,  9, 20,  5],\n",
              "       [20, 10, 29, ..., 10, 18,  4],\n",
              "       ...,\n",
              "       [ 2,  0,  6, ..., 21,  0,  6],\n",
              "       [ 7,  7,  4, ...,  2,  3,  0],\n",
              "       [ 7,  0, 33, ...,  9, 23,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mfGeY9FbNuo",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Architecture\n",
        "Each LSTM cell has 5 parts:\n",
        "1. Input\n",
        "2. prv_state\n",
        "3. prv_output\n",
        "4. new_state\n",
        "5. new_output\n",
        "\n",
        "\n",
        "- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.\n",
        "- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size\n",
        "- An LSTM keeps two pieces of information as it propagates through time: \n",
        "    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n",
        "    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n",
        "\n",
        "\n",
        "#### num_layers = 2 \n",
        "- number of layers in the RNN, is defined by num_layers\n",
        "- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4GStvvHbNup",
        "colab_type": "text"
      },
      "source": [
        "### Defining stacked RNN Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "lMHoAW3abNuq",
        "colab_type": "text"
      },
      "source": [
        "__BasicRNNCell__ is the most basic RNN cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJMSYRYLbNur",
        "colab_type": "code",
        "outputId": "b4ae6201-8f42-4090-c552-d9631a50fa49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0809 02:44:05.834495 140666729650048 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0809 02:44:05.836189 140666729650048 deprecation.py:323] From <ipython-input-9-cbc7d8d66937>:1: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wChFLvsHbNut",
        "colab_type": "code",
        "outputId": "6835e81c-8d33-40a3-c286-352ab45889d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# a two layer cell\n",
        "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0809 02:44:05.855052 140666729650048 deprecation.py:323] From <ipython-input-10-32025279f672>:1: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0809 02:44:05.858805 140666729650048 rnn_cell_impl.py:1642] At least two cells provided to MultiRNNCell are the same object and will share weights.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8nlpbtYbNuv",
        "colab_type": "code",
        "outputId": "4080304c-4978-4574-f66e-5f6426a79c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# hidden state size\n",
        "stacked_cell.output_size"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6mVeEigbNux",
        "colab_type": "text"
      },
      "source": [
        "__state__ varibale keeps output and new_state of the LSTM, so it is a touple of size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EBNSEK6bNuy",
        "colab_type": "code",
        "outputId": "1f2ea5fb-7dd1-40e2-e7a6-736748b728e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "stacked_cell.state_size"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EllpfaTybNu0",
        "colab_type": "text"
      },
      "source": [
        "Lets define input data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-YsOIBebNu1",
        "colab_type": "code",
        "outputId": "1e6b7645-5450-46f2-e699-c4e6fd0da93c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50\n",
        "input_data"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Placeholder_2:0' shape=(60, 50) dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp5_pFi1bNu4",
        "colab_type": "text"
      },
      "source": [
        "and target data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMZ3xkAsbNu6",
        "colab_type": "code",
        "outputId": "822cdca1-eeda-478a-b85c-eec3012a10d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50\n",
        "targets"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Placeholder_3:0' shape=(60, 50) dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guUXXmU6bNvA",
        "colab_type": "text"
      },
      "source": [
        "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
        "\n",
        "__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size\n",
        "representing the batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZSR7NB-bNvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size ? 60x128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2fcHIefebNvG",
        "colab_type": "text"
      },
      "source": [
        "Lets check the value of the input_data again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh59vOWMbNvJ",
        "colab_type": "code",
        "outputId": "4f34338f-3405-4905-a8c2-d1f38439b69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "session = tf.Session()\n",
        "feed_dict={input_data:x, targets:y}\n",
        "session.run(input_data, feed_dict)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[49,  9,  7, ...,  1,  4,  7],\n",
              "       [19,  4, 14, ..., 14,  9, 20],\n",
              "       [ 8, 20, 10, ...,  8, 10, 18],\n",
              "       ...,\n",
              "       [21,  2,  0, ...,  0, 21,  0],\n",
              "       [ 9,  7,  7, ...,  0,  2,  3],\n",
              "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[49,  9,  7, ...,  1,  4,  7],\n",
              "       [19,  4, 14, ..., 14,  9, 20],\n",
              "       [ 8, 20, 10, ...,  8, 10, 18],\n",
              "       ...,\n",
              "       [21,  2,  0, ...,  0, 21,  0],\n",
              "       [ 9,  7,  7, ...,  0,  2,  3],\n",
              "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcLkGGUSbNvO",
        "colab_type": "text"
      },
      "source": [
        "### Embedding\n",
        "In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBcn2cQ4bNvP",
        "colab_type": "text"
      },
      "source": [
        "__Notice:__ The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDAZCHFcbNvQ",
        "colab_type": "code",
        "outputId": "975983b7-1aa4-4014-fbe2-86a4a98fdf5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        }
      },
      "source": [
        "with tf.variable_scope('rnnlm', reuse=False):\n",
        "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
        "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
        "    #with tf.device(\"/cpu:0\"):\n",
        "        \n",
        "    # embedding variable is initialized randomely\n",
        "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
        "\n",
        "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
        "    # it creates a 60*50*[1*128] matrix\n",
        "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
        "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
        "    # split: Splits a tensor into sub tensors.\n",
        "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
        "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
        "    inputs = tf.split(em, seq_length, 1)\n",
        "    # It will convert the list to 50 matrix of [60x128]\n",
        "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0809 02:44:07.156378 140666729650048 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-2932ee7c3adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rnnlm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msoftmax_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax_w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#128x65\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msoftmax_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax_b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1x65)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#with tf.device(\"/cpu:0\"):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable rnnlm/softmax_w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-17-2932ee7c3adb>\", line 2, in <module>\n    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mC8M0IobNvT",
        "colab_type": "text"
      },
      "source": [
        "Lets take a look at the __embedding__, __em__, and __inputs__ variabbles:\n",
        "\n",
        "Embedding variable is initialized with random values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhKRvPNQbNvT",
        "colab_type": "code",
        "outputId": "6d2fe42b-d6c2-43f2-a04e-69222e6a97c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "session.run(tf.global_variables_initializer())\n",
        "#print embedding.shape\n",
        "session.run(embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.10515676,  0.16609721,  0.14853133, ...,  0.15221782,\n",
              "         0.07032926,  0.08683513],\n",
              "       [-0.09505893,  0.1740468 ,  0.00039575, ...,  0.04179668,\n",
              "        -0.04488787, -0.03425205],\n",
              "       [ 0.03661935, -0.14359385, -0.1002887 , ...,  0.07241286,\n",
              "         0.04698692,  0.014552  ],\n",
              "       ...,\n",
              "       [-0.06432796,  0.12638037,  0.09174477, ...,  0.0686183 ,\n",
              "         0.04608312,  0.08143659],\n",
              "       [ 0.13529946, -0.11840481,  0.02908492, ..., -0.13664408,\n",
              "        -0.1443422 ,  0.12054236],\n",
              "       [-0.03822321,  0.03628762,  0.14055218, ..., -0.02858269,\n",
              "         0.1762139 ,  0.03097825]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdQutxeRbNvi",
        "colab_type": "text"
      },
      "source": [
        "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB_hgyUnbNvj",
        "colab_type": "code",
        "outputId": "97442a86-4546-4814-b1ba-ec58dc20a4c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "em = tf.nn.embedding_lookup(embedding, input_data)\n",
        "emp = session.run(em,feed_dict={input_data:x})\n",
        "print emp.shape\n",
        "emp[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60, 50, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.10532202, -0.050962  ,  0.14200698, ..., -0.01312895,\n",
              "         0.0417673 , -0.10974091],\n",
              "       [ 0.08354788,  0.16374655,  0.03215314, ...,  0.05715887,\n",
              "        -0.00169612, -0.16226545],\n",
              "       [-0.04881635, -0.11528797, -0.0641548 , ..., -0.06162267,\n",
              "        -0.03448358, -0.11319518],\n",
              "       ...,\n",
              "       [-0.09505893,  0.1740468 ,  0.00039575, ...,  0.04179668,\n",
              "        -0.04488787, -0.03425205],\n",
              "       [-0.09910723,  0.0268001 , -0.00342618, ..., -0.10508522,\n",
              "         0.08648534,  0.05864352],\n",
              "       [-0.04881635, -0.11528797, -0.0641548 , ..., -0.06162267,\n",
              "        -0.03448358, -0.11319518]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lslWRjGSbNvn",
        "colab_type": "text"
      },
      "source": [
        "Let's consider each sequence as a sentence of length 50 characters, then, the first item in __inputs__ is a [60x128] vector which represents the first characters of 60 sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsDXNh67bNvo",
        "colab_type": "code",
        "outputId": "d3688510-19a1-4a61-e6d9-2baf93b96caa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "inputs = tf.split(em, seq_length, 1)\n",
        "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
        "inputs[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBPV3tLubNvq",
        "colab_type": "text"
      },
      "source": [
        "### Feeding a batch of 50 sequence to a RNN:\n",
        "\n",
        "The feeding process for iputs is as following:\n",
        "\n",
        "- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  \n",
        "- Step 2:  second character of each of the 50 sentences is input in parallel. \n",
        "- Step n: nth character of each of the 50 sentences is input in parallel.  \n",
        "\n",
        "The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSUJElNobNvq",
        "colab_type": "code",
        "outputId": "93a27235-4ce8-4249-b6fb-8b1bf1e301e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "session.run(inputs[0],feed_dict={input_data:x})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.10532202, -0.050962  ,  0.14200698, ..., -0.01312895,\n",
              "         0.0417673 , -0.10974091],\n",
              "       [ 0.15246011,  0.13689722,  0.07332276, ..., -0.13954665,\n",
              "         0.05134563,  0.08537458],\n",
              "       [ 0.02448134, -0.12534583, -0.10239477, ..., -0.00183405,\n",
              "         0.033241  ,  0.02454503],\n",
              "       ...,\n",
              "       [ 0.04817937,  0.13605626,  0.13869353, ...,  0.06541134,\n",
              "         0.09054609, -0.03779064],\n",
              "       [ 0.08354788,  0.16374655,  0.03215314, ...,  0.05715887,\n",
              "        -0.00169612, -0.16226545],\n",
              "       [ 0.00266905, -0.05825555, -0.0354684 , ..., -0.17366202,\n",
              "         0.12272467,  0.14772232]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jperu_KrbNvt",
        "colab_type": "text"
      },
      "source": [
        "Feeding the RNN with one batch, we can check the new output and new state of network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUmWamqlbNvu",
        "colab_type": "code",
        "outputId": "fa4d6cac-2a05-4854-8b83-33f799f5f19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#outputs is 50x[60*128]\n",
        "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
        "new_state"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0809 02:44:09.418715 140666729650048 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YHOqLqbbNvx",
        "colab_type": "code",
        "outputId": "bb2b0860-a901-46ab-a3ac-823541677f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "outputs[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,\n",
              " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1hujbjXbNv0",
        "colab_type": "text"
      },
      "source": [
        "Let's check the output of network after feeding it with first batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttkJiIbNbNv1",
        "colab_type": "code",
        "outputId": "28c49b57-3bd3-4693-809f-26d142bbf47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "first_output = outputs[0]\n",
        "session.run(tf.global_variables_initializer())\n",
        "session.run(first_output,feed_dict={input_data:x})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01865317, -0.11144683, -0.03006576, ...,  0.04930625,\n",
              "        -0.00160041, -0.01676752],\n",
              "       [-0.03037416, -0.02258308, -0.10357679, ..., -0.05289789,\n",
              "        -0.05367209,  0.0380863 ],\n",
              "       [-0.04261411, -0.11313707, -0.02332036, ...,  0.0326977 ,\n",
              "        -0.01151852, -0.01092285],\n",
              "       ...,\n",
              "       [ 0.08442006, -0.05332744, -0.04347272, ..., -0.02535881,\n",
              "        -0.03022373,  0.01977276],\n",
              "       [-0.0684814 ,  0.07481834,  0.02154858, ..., -0.03946618,\n",
              "         0.15615083, -0.0466455 ],\n",
              "       [ 0.0146853 ,  0.06645272,  0.05034451, ..., -0.02579963,\n",
              "        -0.05267765,  0.01782084]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEkWItLtbNv4",
        "colab_type": "text"
      },
      "source": [
        "As it was explained, __outputs__ variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The __softmax_w__ shape is [rnn_size, vocab_size],whihc is [128x65] in our case. Threfore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the __softmax(output * softmax_w + softmax_b)__ for this purpose. The shape of the matrixis would be:\n",
        "\n",
        "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvW7JjbibNv5",
        "colab_type": "text"
      },
      "source": [
        "We can do it step-by-step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz1NlE-ZbNv7",
        "colab_type": "code",
        "outputId": "1da34731-4769-46a7-f2d9-760169b24930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
        "output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDZRqOfSbNv_",
        "colab_type": "code",
        "outputId": "777bfbba-7423-4c36-f26e-263f09244ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "logits = tf.matmul(output, softmax_w) + softmax_b\n",
        "logits"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9KRHdLybNwB",
        "colab_type": "code",
        "outputId": "c1ec0c57-561a-4e44-bb24-1d39307c8ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "probs = tf.nn.softmax(logits)\n",
        "probs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg3IuxhCbNwE",
        "colab_type": "text"
      },
      "source": [
        "Here is the probablity of the next chracter in all batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t_ASQYDbNwF",
        "colab_type": "code",
        "outputId": "cb2c1fe1-cc4a-4f4c-f8a1-ab5e5497d021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "session.run(tf.global_variables_initializer())\n",
        "session.run(probs,feed_dict={input_data:x})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01351181, 0.01894985, 0.01724928, ..., 0.01401815, 0.01509671,\n",
              "        0.01561719],\n",
              "       [0.01207687, 0.016046  , 0.01560649, ..., 0.01688782, 0.01442331,\n",
              "        0.01454099],\n",
              "       [0.01538991, 0.01605703, 0.01702183, ..., 0.01485373, 0.01972784,\n",
              "        0.01738826],\n",
              "       ...,\n",
              "       [0.01246773, 0.02733147, 0.01335132, ..., 0.01508607, 0.01616714,\n",
              "        0.01762396],\n",
              "       [0.01376728, 0.01850387, 0.02263522, ..., 0.02187546, 0.01835082,\n",
              "        0.01668858],\n",
              "       [0.01855796, 0.01469661, 0.01407986, ..., 0.01041665, 0.01255076,\n",
              "        0.0137532 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OofODRZbNwK",
        "colab_type": "text"
      },
      "source": [
        "Now, we are in the position to calculate the cost of training with __loss function__, and keep feedng the network to learn it. But, the question is: what the LSTM networks learn?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGILTShubNwK",
        "colab_type": "code",
        "outputId": "37ea109b-8031-44ab-909f-4981b343b437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "grad_clip =5.\n",
        "tvars = tf.trainable_variables()\n",
        "tvars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
              " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
              " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
              " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
              " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry7l5xWzbNwM",
        "colab_type": "text"
      },
      "source": [
        "# All together\n",
        "Now, let's put all of parts together in a class, and train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs9_kix4bNwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel():\n",
        "    def __init__(self,sample=False):\n",
        "        rnn_size = 128 # size of RNN hidden state vector\n",
        "        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n",
        "        seq_length = 50 # RNN sequence length\n",
        "        num_layers = 2 # number of layers in the RNN\n",
        "        vocab_size = 65\n",
        "        grad_clip = 5.\n",
        "        if sample:\n",
        "            print(\">> sample mode:\")\n",
        "            batch_size = 1\n",
        "            seq_length = 1\n",
        "        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
        "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
        "        # model.cell.state_size is (128, 128)\n",
        "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
        "\n",
        "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
        "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
        "        # Initial state of the LSTM memory.\n",
        "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
        "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
        "\n",
        "        with tf.variable_scope('rnnlm_class1'):\n",
        "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
        "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
        "            with tf.device(\"/cpu:0\"):\n",
        "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
        "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
        "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
        "                #inputs = tf.split(em, seq_length, 1)\n",
        "                \n",
        "                \n",
        "\n",
        "\n",
        "        # The value of state is updated after processing each batch of chars.\n",
        "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
        "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
        "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
        "        self.probs = tf.nn.softmax(self.logits)\n",
        "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
        "                [tf.reshape(self.targets, [-1])],\n",
        "                [tf.ones([batch_size * seq_length])],\n",
        "                vocab_size)\n",
        "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
        "        self.final_state = last_state\n",
        "        self.lr = tf.Variable(0.0, trainable=False)\n",
        "        tvars = tf.trainable_variables()\n",
        "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
        "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
        "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
        "    \n",
        "    \n",
        "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
        "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
        "        #print state\n",
        "        for char in prime[:-1]:\n",
        "            x = np.zeros((1, 1))\n",
        "            x[0, 0] = vocab[char]\n",
        "            feed = {self.input_data: x, self.initial_state:state}\n",
        "            [state] = sess.run([self.final_state], feed)\n",
        "\n",
        "        def weighted_pick(weights):\n",
        "            t = np.cumsum(weights)\n",
        "            s = np.sum(weights)\n",
        "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
        "\n",
        "        ret = prime\n",
        "        char = prime[-1]\n",
        "        for n in range(num):\n",
        "            x = np.zeros((1, 1))\n",
        "            x[0, 0] = vocab[char]\n",
        "            feed = {self.input_data: x, self.initial_state:state}\n",
        "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
        "            p = probs[0]\n",
        "\n",
        "            if sampling_type == 0:\n",
        "                sample = np.argmax(p)\n",
        "            elif sampling_type == 2:\n",
        "                if char == ' ':\n",
        "                    sample = weighted_pick(p)\n",
        "                else:\n",
        "                    sample = np.argmax(p)\n",
        "            else: # sampling_type == 1 default:\n",
        "                sample = weighted_pick(p)\n",
        "\n",
        "            pred = chars[sample]\n",
        "            ret += pred\n",
        "            char = pred\n",
        "        return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ZKzlRmjvbNwP",
        "colab_type": "text"
      },
      "source": [
        "### Creating the LSTM object\n",
        "Now we create a LSTM model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFOoC70ZbNwQ",
        "colab_type": "code",
        "outputId": "dbcd84a4-75ac-4409-8007-2f91c567aaec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "with tf.variable_scope(\"rnn\"):\n",
        "    model = LSTMModel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0809 02:44:12.747869 140666729650048 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/clip_ops.py:286: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEdW5t7qbNwT",
        "colab_type": "text"
      },
      "source": [
        "# Train usinng LSTMModel class\n",
        "We can train our model through feeding batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fHHML6sbNwV",
        "colab_type": "code",
        "outputId": "3b980e23-de58-4ceb-abb1-fb7738b1e050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
        "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
        "        data_loader.reset_batch_pointer()\n",
        "        state = sess.run(model.initial_state) # (2x[60x128])\n",
        "        for b in range(data_loader.num_batches): #for each batch\n",
        "            start = time.time()\n",
        "            x, y = data_loader.next_batch()\n",
        "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
        "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
        "            end = time.time()\n",
        "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
        "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
        "        with tf.variable_scope(\"rnn\", reuse=True):\n",
        "            sample_model = LSTMModel(sample=True)\n",
        "            print sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='The ', sampling_type=1)\n",
        "            print '----------------------------------'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "370/46375 (epoch 0), train_loss = 1.934, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The lif: he ster thre thuek.\n",
            "\n",
            "SATY UTESO\n",
            "BENTUL:\n",
            "I, of\n",
            "----------------------------------\n",
            "741/46375 (epoch 1), train_loss = 1.779, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The are\n",
            "sucker, there my loath, abodes\n",
            "I water's denw-\n",
            "----------------------------------\n",
            "1112/46375 (epoch 2), train_loss = 1.704, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The it. Heret with, nesty it\n",
            "hum? I at iur not that is\n",
            "----------------------------------\n",
            "1483/46375 (epoch 3), train_loss = 1.656, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The other you bern;\n",
            "Qur not.\n",
            "\n",
            "GREY:\n",
            "Set that happile\n",
            "T\n",
            "----------------------------------\n",
            "1854/46375 (epoch 4), train_loss = 1.623, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The ears: the must I saces son,\n",
            "I'll thou was pitife\n",
            "T\n",
            "----------------------------------\n",
            "2225/46375 (epoch 5), train_loss = 1.598, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The wenfer tell..\n",
            "\n",
            "WARWICK:\n",
            "Nobbawted that not proves \n",
            "----------------------------------\n",
            "2596/46375 (epoch 6), train_loss = 1.578, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The woed\n",
            "Follow him.\n",
            "Here is fawe, as last talk'd sinc\n",
            "----------------------------------\n",
            "2967/46375 (epoch 7), train_loss = 1.563, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The Wass wail itate, my morthess bonouge? O, never see\n",
            "----------------------------------\n",
            "3338/46375 (epoch 8), train_loss = 1.551, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The woman.\n",
            "\n",
            "NARELEON:\n",
            "My droop'd with libertays sife y\n",
            "----------------------------------\n",
            "3709/46375 (epoch 9), train_loss = 1.541, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The succure yet put my lova, as we none's, bittle, be \n",
            "----------------------------------\n",
            "4080/46375 (epoch 10), train_loss = 1.532, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The more glavet,\n",
            "Sir, with reportanding thy virtue of \n",
            "----------------------------------\n",
            "4451/46375 (epoch 11), train_loss = 1.526, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The liment: king! see access to elor.\n",
            "\n",
            "First Angeloin?\n",
            "----------------------------------\n",
            "4822/46375 (epoch 12), train_loss = 1.520, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The cominiurs, sure proud.\n",
            "\n",
            "KING HENRY VI:\n",
            "My heaven\n",
            "t\n",
            "----------------------------------\n",
            "5193/46375 (epoch 13), train_loss = 1.515, time/batch = 0.012\n",
            ">> sample mode:\n",
            "The must nakedid younger dusting I say.\n",
            "\n",
            "GRETHO:\n",
            "Ag ne\n",
            "----------------------------------\n",
            "5564/46375 (epoch 14), train_loss = 1.510, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The pleasure.\n",
            "Though be call, he that is tife\n",
            "To hear \n",
            "----------------------------------\n",
            "5935/46375 (epoch 15), train_loss = 1.505, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The here said.\n",
            "O, not to thy loves, and frown hail wit\n",
            "----------------------------------\n",
            "6306/46375 (epoch 16), train_loss = 1.501, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The and course to slargege\n",
            "and, but,\n",
            "For super's his:\n",
            "\n",
            "----------------------------------\n",
            "6677/46375 (epoch 17), train_loss = 1.497, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The mine\n",
            "The newly, gentle's small't wit, he true nigh\n",
            "----------------------------------\n",
            "7048/46375 (epoch 18), train_loss = 1.493, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The white ever stade me, good my lover Ramnant\n",
            "thou th\n",
            "----------------------------------\n",
            "7419/46375 (epoch 19), train_loss = 1.490, time/batch = 0.017\n",
            ">> sample mode:\n",
            "The house wincess toath to be that are conceech heaven\n",
            "----------------------------------\n",
            "7790/46375 (epoch 20), train_loss = 1.488, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The wall--wert them in needful.\n",
            "\n",
            "First Servingman:\n",
            "And\n",
            "----------------------------------\n",
            "8161/46375 (epoch 21), train_loss = 1.485, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The good my book orast him with patient watch,\n",
            "And he \n",
            "----------------------------------\n",
            "8532/46375 (epoch 22), train_loss = 1.482, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The more this bold, my true\n",
            "Norfolk, live\n",
            "Her obearlei\n",
            "----------------------------------\n",
            "8903/46375 (epoch 23), train_loss = 1.480, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The mead'st tito help of such marrixtcharn; my laid\n",
            "An\n",
            "----------------------------------\n",
            "9274/46375 (epoch 24), train_loss = 1.478, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The king,\n",
            "For engignimer us in husband was moveing him\n",
            "----------------------------------\n",
            "9645/46375 (epoch 25), train_loss = 1.475, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The will be one re's play is cousin will frole, this b\n",
            "----------------------------------\n",
            "10016/46375 (epoch 26), train_loss = 1.473, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The persing of your grace!\n",
            "\n",
            "CLARENCE:\n",
            "I thank is witha\n",
            "----------------------------------\n",
            "10387/46375 (epoch 27), train_loss = 1.471, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The younger at any other\n",
            "For farewell seee the rive me\n",
            "----------------------------------\n",
            "10758/46375 (epoch 28), train_loss = 1.469, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The ears,\n",
            "For Edawdy,\n",
            "I poor bloody that me, his nurse\n",
            "----------------------------------\n",
            "11129/46375 (epoch 29), train_loss = 1.467, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The ungassing,--\n",
            "Buting of from Axcuse meek is the lik\n",
            "----------------------------------\n",
            "11500/46375 (epoch 30), train_loss = 1.465, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The murderer, what-due?\n",
            "\n",
            "MERCUTIO:\n",
            "Fie; Angelo, theref\n",
            "----------------------------------\n",
            "11871/46375 (epoch 31), train_loss = 1.463, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The haught simpliaft, and time: here I have I bear the\n",
            "----------------------------------\n",
            "12242/46375 (epoch 32), train_loss = 1.462, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The high ten that\n",
            "hanger but true three had, thou hast\n",
            "----------------------------------\n",
            "12613/46375 (epoch 33), train_loss = 1.460, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The looks;\n",
            "I'll not some wretch'd to our this deXt!\n",
            "\n",
            "P\n",
            "----------------------------------\n",
            "12984/46375 (epoch 34), train_loss = 1.459, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The beast, belose, as passond\n",
            "in the sused and like to\n",
            "----------------------------------\n",
            "13355/46375 (epoch 35), train_loss = 1.458, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The now, deser\n",
            "On languoo.\n",
            "\n",
            "BRUTUS:\n",
            "Give you thy reque\n",
            "----------------------------------\n",
            "13726/46375 (epoch 36), train_loss = 1.456, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The newb with curse to till to formosted him carm our \n",
            "----------------------------------\n",
            "14097/46375 (epoch 37), train_loss = 1.455, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The world to our deniould we beghall our wickernt,\n",
            "His\n",
            "----------------------------------\n",
            "14468/46375 (epoch 38), train_loss = 1.454, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The need Ty counsel: nor way?\n",
            "Forgot. For a way;\n",
            "Who t\n",
            "----------------------------------\n",
            "14839/46375 (epoch 39), train_loss = 1.453, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The hour pleasure and Richard my lord, but opt, as you\n",
            "----------------------------------\n",
            "15210/46375 (epoch 40), train_loss = 1.452, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The woman such your mean the every, husbands: old Joza\n",
            "----------------------------------\n",
            "15581/46375 (epoch 41), train_loss = 1.451, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The gates, George; my munder of every perpes of a coum\n",
            "----------------------------------\n",
            "15952/46375 (epoch 42), train_loss = 1.450, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The liks:\n",
            "You are prove in his subbly laughther\n",
            "As thr\n",
            "----------------------------------\n",
            "16323/46375 (epoch 43), train_loss = 1.449, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The speechs murder it busy wore;\n",
            "Your own will:\n",
            "Farewi\n",
            "----------------------------------\n",
            "16694/46375 (epoch 44), train_loss = 1.448, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The Richary for this coming of cousin; my lord, on the\n",
            "----------------------------------\n",
            "17065/46375 (epoch 45), train_loss = 1.447, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The firm off, sir, no, I pray to give Coriolar; bornaf\n",
            "----------------------------------\n",
            "17436/46375 (epoch 46), train_loss = 1.446, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The earms:\n",
            "My masterly of alone\n",
            "And Cain the weddurn c\n",
            "----------------------------------\n",
            "17807/46375 (epoch 47), train_loss = 1.445, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The munstinn done.\n",
            "But, whispen cank up thee:\n",
            "\n",
            "KING ED\n",
            "----------------------------------\n",
            "18178/46375 (epoch 48), train_loss = 1.444, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The royal you to bed, I will cursed the Camms, that af\n",
            "----------------------------------\n",
            "18549/46375 (epoch 49), train_loss = 1.443, time/batch = 0.012\n",
            ">> sample mode:\n",
            "The and our mourn thy blood,\n",
            "To good gentle likeluse l\n",
            "----------------------------------\n",
            "18920/46375 (epoch 50), train_loss = 1.443, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The fear\n",
            "A Frantless he shall pity toward in Nellow, w\n",
            "----------------------------------\n",
            "19291/46375 (epoch 51), train_loss = 1.442, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The matted of God;\n",
            "That you goes?\n",
            "Whilst with at us;\n",
            "A\n",
            "----------------------------------\n",
            "19662/46375 (epoch 52), train_loss = 1.441, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The modest is their own laughter, with forchiancelving\n",
            "----------------------------------\n",
            "20033/46375 (epoch 53), train_loss = 1.440, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The into the man\n",
            "thing: now,\n",
            "Which so I will? how of m\n",
            "----------------------------------\n",
            "20404/46375 (epoch 54), train_loss = 1.440, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The outest from this highorlided our just not boar, th\n",
            "----------------------------------\n",
            "20775/46375 (epoch 55), train_loss = 1.439, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The lies, signer.\n",
            "\n",
            "Mespreator; and this wethouddeps my\n",
            "----------------------------------\n",
            "21146/46375 (epoch 56), train_loss = 1.438, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The antertaina, where, commitch and drown\n",
            "as thanks, o\n",
            "----------------------------------\n",
            "21517/46375 (epoch 57), train_loss = 1.438, time/batch = 0.015\n",
            ">> sample mode:\n",
            "The magar obks I am one no mores, seen than sit in tru\n",
            "----------------------------------\n",
            "21888/46375 (epoch 58), train_loss = 1.437, time/batch = 0.012\n",
            ">> sample mode:\n",
            "The painted on a leach in their grave but all hung,\n",
            "Ex\n",
            "----------------------------------\n",
            "22259/46375 (epoch 59), train_loss = 1.437, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The sworn tims\n",
            "his sturt. Come, Such 'Gire.\n",
            "\n",
            "LEONTES:\n",
            "\n",
            "----------------------------------\n",
            "22630/46375 (epoch 60), train_loss = 1.436, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The amalics\n",
            "Warlow'd than say good thought up to come \n",
            "----------------------------------\n",
            "23001/46375 (epoch 61), train_loss = 1.435, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The saffey.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Go good of with we sunnig\n",
            "----------------------------------\n",
            "23372/46375 (epoch 62), train_loss = 1.435, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The measumence or elserearing mase: need and with with\n",
            "----------------------------------\n",
            "23743/46375 (epoch 63), train_loss = 1.434, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The strange you good Peiryer; make a baush,\n",
            "Subles of \n",
            "----------------------------------\n",
            "24114/46375 (epoch 64), train_loss = 1.434, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The in the grave,\n",
            "To Joht Plucklucking hast, as years,\n",
            "----------------------------------\n",
            "24485/46375 (epoch 65), train_loss = 1.433, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The them, I have it: his he;\n",
            "Good here, one rock he is\n",
            "----------------------------------\n",
            "24856/46375 (epoch 66), train_loss = 1.433, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The anathey; if I'er Deep!\n",
            "Detorded good\n",
            "Youngetel,\n",
            "He\n",
            "----------------------------------\n",
            "25227/46375 (epoch 67), train_loss = 1.433, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The wager: you wake--\n",
            "You dew rosont;\n",
            "And that have my\n",
            "----------------------------------\n",
            "25598/46375 (epoch 68), train_loss = 1.432, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The hell:\n",
            "Whence to the people?\n",
            "\n",
            "DUKE OF YORK:\n",
            "You are\n",
            "----------------------------------\n",
            "25969/46375 (epoch 69), train_loss = 1.432, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The good mercy,\n",
            "and like his suns' is distracted.\n",
            "\n",
            "LUC\n",
            "----------------------------------\n",
            "26340/46375 (epoch 70), train_loss = 1.431, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The sad away ourselves, to near is honour for thy taad\n",
            "----------------------------------\n",
            "26711/46375 (epoch 71), train_loss = 1.431, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The placter is Angelo men back:--\n",
            "\n",
            "CORIOLANUS:\n",
            "Forscen\n",
            "----------------------------------\n",
            "27082/46375 (epoch 72), train_loss = 1.430, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The better there that we have head;\n",
            "As news, like his \n",
            "----------------------------------\n",
            "27453/46375 (epoch 73), train_loss = 1.430, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The in breath.\n",
            "\n",
            "Messenger:\n",
            "Perding us must justice: so\n",
            "----------------------------------\n",
            "27824/46375 (epoch 74), train_loss = 1.430, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The thou prove.\n",
            "\n",
            "GLOUCENTIO:\n",
            "I thank, breathin those n\n",
            "----------------------------------\n",
            "28195/46375 (epoch 75), train_loss = 1.429, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The since\n",
            "Or well scale at Gaunt?\n",
            "Our plack we have no\n",
            "----------------------------------\n",
            "28566/46375 (epoch 76), train_loss = 1.429, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The beace?\n",
            "\n",
            "GONZALO:\n",
            "Believe her worst:\n",
            "And thou, must\n",
            "----------------------------------\n",
            "28937/46375 (epoch 77), train_loss = 1.429, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The natures,\n",
            "To your lord with mind.\n",
            "\n",
            "GREMIO:\n",
            "Any curs\n",
            "----------------------------------\n",
            "29308/46375 (epoch 78), train_loss = 1.428, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The have poor pooking with late;\n",
            "Thus he been aboof'y,\n",
            "----------------------------------\n",
            "29679/46375 (epoch 79), train_loss = 1.428, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The spite interior;\n",
            "And alack awaken is her thou shalt\n",
            "----------------------------------\n",
            "30050/46375 (epoch 80), train_loss = 1.428, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The swear,\n",
            "Have not a thrade the great,\n",
            "A man dead the\n",
            "----------------------------------\n",
            "30421/46375 (epoch 81), train_loss = 1.427, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The death; and then, thanky shame, childre govereight \n",
            "----------------------------------\n",
            "30792/46375 (epoch 82), train_loss = 1.427, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The day and\n",
            "with my fought of delicue:\n",
            "Have do ask and\n",
            "----------------------------------\n",
            "31163/46375 (epoch 83), train_loss = 1.427, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The silver, what is she will be weak here, here's inde\n",
            "----------------------------------\n",
            "31534/46375 (epoch 84), train_loss = 1.427, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The him, I am asked a night\n",
            "Upon, the gets with a joys\n",
            "----------------------------------\n",
            "31905/46375 (epoch 85), train_loss = 1.426, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The most hour more perpit is not our seems confeilture\n",
            "----------------------------------\n",
            "32276/46375 (epoch 86), train_loss = 1.426, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The good save it being all the sea!\n",
            "\n",
            "GLOUCESTER:\n",
            "Well.\n",
            "----------------------------------\n",
            "32647/46375 (epoch 87), train_loss = 1.426, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The laying you done.\n",
            "I will not days be, I dares are s\n",
            "----------------------------------\n",
            "33018/46375 (epoch 88), train_loss = 1.425, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The arried,--\n",
            "\n",
            "BENVIO:\n",
            "Her not well; sir, with the cha\n",
            "----------------------------------\n",
            "33389/46375 (epoch 89), train_loss = 1.425, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The win the worst that say with him imposings to make \n",
            "----------------------------------\n",
            "33760/46375 (epoch 90), train_loss = 1.425, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The coming table,\n",
            "We are scare, a very,\n",
            "If the fold wi\n",
            "----------------------------------\n",
            "34131/46375 (epoch 91), train_loss = 1.425, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The lazy, what is neutch'd is as the colours well to h\n",
            "----------------------------------\n",
            "34502/46375 (epoch 92), train_loss = 1.425, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The cruel, what; but not stonest I wish of pitishone: \n",
            "----------------------------------\n",
            "34873/46375 (epoch 93), train_loss = 1.424, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The shavert:\n",
            "Then, the his statics!\n",
            "\n",
            "BIONDELLO:\n",
            "Why, a\n",
            "----------------------------------\n",
            "35244/46375 (epoch 94), train_loss = 1.424, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The now.\n",
            "\n",
            "LADY:\n",
            "Ay, when the amgness my glorious lord?\n",
            "----------------------------------\n",
            "35615/46375 (epoch 95), train_loss = 1.424, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The suster?\n",
            "\n",
            "MENENIUS:\n",
            "So much is our spring youth tur\n",
            "----------------------------------\n",
            "35986/46375 (epoch 96), train_loss = 1.424, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The war your love to live this state drunking proud to\n",
            "----------------------------------\n",
            "36357/46375 (epoch 97), train_loss = 1.423, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The coached\n",
            "Yond's. I be like one ask or by my house y\n",
            "----------------------------------\n",
            "36728/46375 (epoch 98), train_loss = 1.423, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The with inferous; and then?\n",
            "\n",
            "GRUMIO:\n",
            "Lord hastly.\n",
            "Ah,\n",
            "----------------------------------\n",
            "37099/46375 (epoch 99), train_loss = 1.423, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The write\n",
            "Decels.\n",
            "Some man he satter'd grave.\n",
            "\n",
            "Second \n",
            "----------------------------------\n",
            "37470/46375 (epoch 100), train_loss = 1.423, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The more than but speak\n",
            "Than thy left him\n",
            "In's brother\n",
            "----------------------------------\n",
            "37841/46375 (epoch 101), train_loss = 1.423, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The know, with use at you: the deep a triusiness\n",
            "The c\n",
            "----------------------------------\n",
            "38212/46375 (epoch 102), train_loss = 1.423, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The my loss,\n",
            "And Bianst bid!\n",
            "Take you?\n",
            "\n",
            "GREY:\n",
            "If a hap\n",
            "----------------------------------\n",
            "38583/46375 (epoch 103), train_loss = 1.422, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The megred truth.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Stainsel or what a\n",
            "----------------------------------\n",
            "38954/46375 (epoch 104), train_loss = 1.422, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The steel; then, for them; nut rest? why thou art faul\n",
            "----------------------------------\n",
            "39325/46375 (epoch 105), train_loss = 1.422, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The hand,\n",
            "To glor; if it a dute of shall ply the distr\n",
            "----------------------------------\n",
            "39696/46375 (epoch 106), train_loss = 1.422, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The one, and be foother.\n",
            "Mine-counterancelant us and a\n",
            "----------------------------------\n",
            "40067/46375 (epoch 107), train_loss = 1.422, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The stately very\n",
            "sins-beging towbel 'Tis a false:\n",
            "'Tis\n",
            "----------------------------------\n",
            "40438/46375 (epoch 108), train_loss = 1.421, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The of thyswear of his affit,\n",
            "Enswy.\n",
            "Call pardon the a\n",
            "----------------------------------\n",
            "40809/46375 (epoch 109), train_loss = 1.421, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The bearner as wantholb arms of these father's love?\n",
            "\n",
            "\n",
            "----------------------------------\n",
            "41180/46375 (epoch 110), train_loss = 1.421, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The devise and man to the mesturns,\n",
            "But I am he hath l\n",
            "----------------------------------\n",
            "41551/46375 (epoch 111), train_loss = 1.421, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The does.\n",
            "\n",
            "ELBOW:\n",
            "Beseech you am by a pusterly lose\n",
            "To\n",
            "----------------------------------\n",
            "41922/46375 (epoch 112), train_loss = 1.421, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The straw it is not two good-mace,\n",
            "To make you lacked \n",
            "----------------------------------\n",
            "42293/46375 (epoch 113), train_loss = 1.421, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The and a care:\n",
            "Too masters are shall be foe my mistor\n",
            "----------------------------------\n",
            "42664/46375 (epoch 114), train_loss = 1.420, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The name:\n",
            "The good word.\n",
            "\n",
            "KING RICHARD II:\n",
            "That I wife\n",
            "----------------------------------\n",
            "43035/46375 (epoch 115), train_loss = 1.420, time/batch = 0.015\n",
            ">> sample mode:\n",
            "The tied,\n",
            "You, that this: the very\n",
            "sigh bound,\n",
            "By my t\n",
            "----------------------------------\n",
            "43406/46375 (epoch 116), train_loss = 1.420, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The earth,\n",
            "Is mean more!\n",
            "\n",
            "GONZALO:\n",
            "Never govern Marciu\n",
            "----------------------------------\n",
            "43777/46375 (epoch 117), train_loss = 1.420, time/batch = 0.014\n",
            ">> sample mode:\n",
            "The sweat! deliver\n",
            "If you on my brother\n",
            "The must make \n",
            "----------------------------------\n",
            "44148/46375 (epoch 118), train_loss = 1.420, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The noble four down.\n",
            "This along.\n",
            "\n",
            "ISABELLA:\n",
            "It underst\n",
            "----------------------------------\n",
            "44519/46375 (epoch 119), train_loss = 1.420, time/batch = 0.012\n",
            ">> sample mode:\n",
            "The wretch,\n",
            "That iring alamonest very bed in thy senat\n",
            "----------------------------------\n",
            "44890/46375 (epoch 120), train_loss = 1.420, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The heresous crown! the crossed strongs;\n",
            "His new despa\n",
            "----------------------------------\n",
            "45261/46375 (epoch 121), train_loss = 1.419, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The cause. or be corven 'cate:\n",
            "My father, so so\n",
            "Her ha\n",
            "----------------------------------\n",
            "45632/46375 (epoch 122), train_loss = 1.419, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The thou then I will valiant's truth of followen\n",
            "firmo\n",
            "----------------------------------\n",
            "46003/46375 (epoch 123), train_loss = 1.419, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The assmixttigain.\n",
            "The blood will to your sweat of bei\n",
            "----------------------------------\n",
            "46374/46375 (epoch 124), train_loss = 1.419, time/batch = 0.013\n",
            ">> sample mode:\n",
            "The than what is so Hust like at me.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "lYiw-iu_bNwY",
        "colab_type": "text"
      },
      "source": [
        "# Thanks for completing this lesson!\n",
        "Created by: <a href = \"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a></h4>\n",
        "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyt5VMuIcLho",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "<p>Copyright &copy; 2017 IBM <a href=\"https://cognitiveclass.ai/?utm_source=ML0151&utm_medium=lab&utm_campaign=cclab\">IBM Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href=\"https://cognitiveclass.ai/mit-license/\">MIT License</a>.</p>"
      ]
    }
  ]
}